% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.


% Source 1 - Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and AI Interaction Design
@inproceedings{reflexiveprompt,
author = {Djeffal, Christian},
title = {Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and AI Interaction Design},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732118},
doi = {10.1145/3715275.3732118},
abstract = {Responsible prompt engineering has emerged as a critical pracitce for ensuring that generative artificial intelligence (AI) systems are aligned with ethical, legal, and social principles. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. It is, therefore, necessary to examine how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes “Reflexive Prompt Engineering”, a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of emerging practices, this article illustrates how responsible prompt engineering serves as a crucial connection between AI development and deployment, enabling organizations to align AI outputs without modifying underlying model architectures. This approach links with broader “Responsibility by Design” principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering as an essential component of AI literacy.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1757-1768},
numpages = {12},
keywords = {AI Ethics, AI Governance, AI alignment, Accountability, Human-AI Interaction, Prompt Engineering, Responsible AI},
location = {},
series = {FAccT '25},
annote = {When working within AI, it is imperative to know that effective prompting not only relies on a technically sound system to generate safe content from prompts but must also include ethically minded efforts to maintain true, meaning safe, functionality. This source talks on how the world of AI systems currently do, or do not, oversee this ethical quandary beyond the scope of their technical architects making them, as the matter of prompt engineering still lacks competent frameworks from which to guide prompt engineers’ practices properly. Further, their research question pertained to three aspects primarily: examining the necessary parts of prompt engineering practices to showcase the depth deployers may engage in when crafting the system’s output, exploring how these practices enhance different/multi-context use (e.g. math versus writing), and analyzing the current gaps in processes while seeking opportunities to brighten AI responsibility in deployment, some of which is rising independently. This source took effort in gathering results, going through academic databases and targeted searches with the methodology described above that include: arXiv, IEEE, Xplore, ACM Digital Library, Google Scholar, DuckDuckGo, and Semantic Scholar. This source comes with 130 different references, so there was a good amount of research backing this literary review. A summarization of the source’s content belies that responsible prompt engineering practices manage five points, of which make up their definition of reflexive prompt engineering, stated again in sequence: first on prompt design, second being system selection, third involving the systems configuration, fourth evaluating the performance, and fifth being the actual management of the prompt. The paradigms of reflexive prompt engineering will influence the prompt design framework for making the AI Hub currently ideated, the goal being to have an ethical system to neither marginalize, nor misconstrue, its users when interacting, considering how general and customizable this Hub should be.}
}

% SOURCE 2 - Agent-Initiated Interaction in Phone UI Automation
@inproceedings{phoneui,
author = {Kahlon, Noam and Rom, Guy and Efros, Anatoly and Galgani, Filippo and Berkovitch, Omri and Caduri, Sapir and Bishop, William E. and Riva, Oriana and Dagan, Ido},
title = {Agent-Initiated Interaction in Phone UI Automation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717526},
doi = {10.1145/3701716.3717526},
abstract = {Phone automation agents aim to autonomously perform a given natural-language user request, such as scheduling appointments or booking a hotel. While much research effort has been devoted to screen understanding and action planning, complex tasks often necessitate user interaction for successful completion. Aligning the agent with the user's expectations is crucial for building trust and enabling personalized experiences. This requires the agent to proactively engage the user when necessary, avoiding actions that violate their preferences while refraining from unnecessary questions where a default action is expected. We argue that such subtle agent-initiated interaction with the user deserves focused research attention.To promote such research, this paper introduces a task formulation for detecting the need for user interaction and generating appropriate messages. We thoroughly define the task, including aspects like interaction timing and the scope of the agent's autonomy. Using this definition, we derived annotation guidelines and created a diverse dataset for the task, leveraging an existing UI automation dataset. We tested several text-based and multimodal baseline models for the task, finding that it is very challenging for current LLMs. We suggest that our task formulation, dataset, baseline models and analysis will be valuable for future UI automation research, specifically in addressing this crucial yet often overlooked aspect of agent-initiated interaction. This work provides a needed foundation to allow personalized agents to properly engage the user when needed, within the context of phone UI automation.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2391-2400},
numpages = {10},
keywords = {conversational interaction, dataset, phone ui automation},
location = {Sydney NSW, Australia},
series = {WWW '25},
annote = {Agentic Ai has taken the world by a semi-overly hyped storm as of the time of writing, so research into how an Agentic AI could act like the ones seen within science fiction, take Cyberpunk 2077 for example, is only stopped by the lack of research time or researching other topics prior. This is not the first paper to study this topic in some manner, as the authors state themselves, and such research, involving multi-modal conversation agents on mobile graphical user interfaces (GUI), or otherwise similar themes, is found throughout all their twenty-eight references. Each effort made by a group only pushes closer to the end goal. With a personal topic of study being undertaken this semester involving designing and creating an Agentic AI Hub, a one-stop-customizable-prompt-shop for user needs, this paper is imperative for providing past research data, processes and paths traveled. Discerning what this paper specifically brings to the table involves explaining the end goal of these Agent products. At present, Phone UI automations act to the benefit of its user’s interests, either by completing tasks of scheduling automatically, or reading text from a menu aloud for the visually impaired; all is provided through natural language (NL) instruction and starting screen and is an idea which has been personally designed and still used to this day. The paper describes how most current methods rely on a phone’s user interface (UI) metadata to then act through a series of GUI instructions. The issue then, is the confines with which the product must work within, which is what an Agent initiated interaction hopes to escape in the end. The end goal of designing this product involves being able to clearly define, update, and restrain an Agent to align a deep understanding of the user’s needs with the Agent’s functionality. Stated in simpler terms, the Agent should not recommend a user with a known peanut allergy to eat Szechuan chicken at a restaurant; not without warning that it may or may not contain the peanut sauce first, or better yet, could, with permissions prior set from the user, research the restaurant the user is eating at to determine, if possible, whether the food contains contaminants or not. This is not a straightforward process in the slightest for current AI models though, as expressed by one example shown in Table 1 of the paper explaining how the task of identifying a plant in a picture could lead to follow-up question after follow-up question. The source created datasets through having humans emulate an agent, acting on no prior held knowledge than that given in the prompt, and used their successive questions to simulate the running of an Agent. While this works okay in some simpler instances, multi-turn aspects proved too complex for what was available in trying on Android datasets and later found this true also in general AI models when prompted; their thinking would be so rigid, that many questions would be needed to reach a clear enough 'context’ that the real work could begin. Beyond functionality is the basic question of ethics as well; an Agent could quite easily help someone commit malicious acts without proper framework preventions, and even then, if the Agent is finally created with flexibility, there may yet prove too much modularity in user needs for it to constantly safeguard itself, lest a nuanced, likely reflexive and relational, method of 'bad-action' catching is employed beyond good and bad points.}
}

% Source 3: Building Appropriate Mental Models: What users know and want to know about an Agentic AI Chatbot
@inproceedings{mentalmodel,
author = {Brachman, Michelle and Kunde, Siya and Miller, Sarah and Fucs, Ana and Dempsey, Samantha and Jabbour, Jamie and Geyer, Werner},
title = {Building Appropriate Mental Models: What Users Know and Want to Know about an Agentic AI Chatbot},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712071},
doi = {10.1145/3708359.3712071},
abstract = {Agentic systems aim to handle complex problems with increasing system autonomy using generative AI. These new agentic systems are becoming more feasible and easier to build. Yet we know little about what end-users need to know to use these systems appropriately. We study one such agentic system, “Gent,” which can break down complex problems into a set of actions, provide a rationale for each action, interact with external information, and cite its sources. Our goals were to understand users' mental models of the agentic system, the information users leveraged to evaluate the accuracy of the system, and users' information needs. In our study (N=24), participants interacted with Gent for four information seeking tasks where they could see Gent's actions, rationale, and sources. Participants' mental models centered around the search-like qualities of the system, with their confidence impacted by the website sources. Participants' mental models often lacked insight into the workings of the generative AI model and agentic framework that impact the actions the system takes. Participants used the descriptions of the system's actions to support their evaluation of the accuracy of the system and wanted to know more about how the system got to its answers. Participants also relied on their own personal knowledge and the style or length of Gent's responses to evaluate the accuracy. Our results highlight the need for further transparency in agentic AI systems to support end-users in evaluating system outputs and help them build effective mental models.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {247-264},
numpages = {18},
keywords = {Agentic AI, Generative AI, Conversational UI, Information Seeking, Transparency, Explainable AI, Mental Models, Reliance},
location = {},
series = {IUI '25},
annote = {The use of an Agentic AI, whether as a chatbot between a user and service provider, or actionable sequences to perform a function of a service on its own, is a newly researched, and increasingly feasible, fledgling field of study that governs the pathway to formerly science fiction ideas of possibilities behind what Siri or beyond could do. This source aims to answer one of the two crucial questions behind how to make this product work; while on one side it is important to make the AI work within the parameters specific to the user's needs, and not move beyond them, particularly in critical matters of data/internet safety, there is an untapped factor about how to help the user actually use this Agentic AI, especially when relating to chatbots. This source used the agentic system “Gent” to have 24 participants perform four information seeking tasks, as Gent is basically a summarizer and citation provider, and catalogued their mental models behind how to work the agentic system to their benefit. It was found, as is common, that most lacked insight into the generative AI model and agentic framework's inner-workings, unable to properly maximize the system's processes to gain any benefits. The outcome of their tests highlighted a need to agentic AI system sot support end-users in examining system outputs to improve each outcome and help build more effective mental models of how to navigate them via a transparent 'guide', in essence. More clearly said, agentic models should help their end-users navigate themselves easier and efficiently. For example, not all users discern the accuracy of an agentic system the same, as shown by only four members of the study recognizing that the large language model (LLM) has its own library of information from pre-training it uses to generate answers beyond what it searches through Google and Wikipedia application programming interfaces’ (API’s). This paper provides deep insights into the design challenges that can be faced regarding how an end-user may properly, or improperly, use and/or understand the product. When designing an Agentic AI Hub, this is significant to the process of trying to make sure the user understands how the information is gathered, used, and stored between what request of an action is to be undertaken, and the output reached, including what steps were needed for the likely next ask of the user in the process due to the current human direction dependance still apparent in  agentic AI systems. The mental model behind how to use the Hub will heavily, heavily determine what a person may expect from the application. Especially regarding the accuracy of the product. Having tools hidden even behind one click of a menu can diminish visibility to a debilitating degree. An important takeaway taken from this paper is their data on the information an end-user wants to know about how an agentic AI chatbot works. Most of the questions pertained to the AI’s logic and reasoning, with some asking on the sources, and others on how to control the tool, and the AI’s self-confidence in answering. The general idea behind these questions was a lack of transparency; sources did not specify how they contributed to its final answer, or were weighted in importance for answer generation, and their credibility. The idea of transparency has a tricky balance to it; over-explanation erodes user trust, so critical insights only should be shown to the user to help their understanding of what the output is. This is a small-scale study, so data insights can only be taken so far, but what is available is well researched and clarified through interviews of the participants, cataloguing of the data, and furthered critical analysis of prior completed related studies.}
}

% Source 4 - Requirements Are All You Need: The Final Frontier for End-User Software Engineering
@article{finalfrontier,
author = {Robinson, Diana and Cabrera, Christian and Gordon, Andrew D. and Lawrence, Neil D. and Mennen, Lars},
title = {Requirements Are All You Need: The Final Frontier for End-User Software Engineering},
year = {2025},
month= {June},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708524},
doi = {10.1145/3708524},
abstract = {What if end-users could own the software development lifecycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative AI brings to software generation and maintenance techniques. How could designing software in this way better serve end-users? What are the implications of this process for the future of end-user software engineering and the software development lifecycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future.},
journal = {ACM Trans. Softw. Eng. Methodol.},
articleno = {141},
numpages = {22},
keywords = {End-User Software Engineering, End-User Programming, Large Language Models},
annote = {This source tries to answer the questions proposed by other papers and documentation regarding the proper use of generative AI when it reaches the end-user; what requirements make using a generative AI more standardized, or at least understandable and common, regarding end-user and AI's transparent framework between the heaving end-user's needs understood and the generative AI's prompting and programming clear-and-unproblematic. This article's goal is not only to make the generative AI's use, and possible consequences, more transparent, but to aid in the general software development lifecycle process an end-user may unwittingly already be involved in; modern-day technical literacy in the business field, and otherwise, may include the use of specialized programming in a domain of expertise (e.g. making time-planned sheets), which is a field denoted as End-User-Software-Engineering (EUSE). The article states their theory that in 2030, EUSE will shift to become requirements-driven, which basically is the idea of human-AI interfaces designed around using large language models (LLM), as the article is arguing, to make apps through a development team simulated by AI, followed up by expert human oversight to mitigate costly mistakes. However, the practices studied here are currently withheld by two hindrances. The first is how human users are not innately interested in expressing the requirements behind a task or app request in natural language, even if it may be a more manageable medium than the average programming language. Second includes how machines that interpret natural language are not inherently trustworthy, as the answer they push as correct is not always true. This new lifecycle can also be split numerically, the article detailing the resulting data their case study tested, including three meaningful chunks to the requirements-driven EUSE process: requirements elicitation, testing, and maintenance & deployment. All of these are cyclic in nature, repeating until the app is created. Requirements elicitation is the proposed paradigm to assist an end-user in expressing their requirements to the generative-AI; the goal is to surmount the hurdles regarding an end-user sharing their requirements clearly enough for an automated process to pick up, and to keep the actual build limitations and scope clear as well. With two works of EUSE cited specifically, programming by example, and natural language programming, these are the two parts of the article which begin to relate most heavily the project idea regarding an agentic AI hub, if this hub will be a workspace for code to be generated in from the requirements stated by a user to create a nifty and personalized automated workflow.}
}

% Source 5 - INSYTE: A Classification Framework for Traditional to Agentic AI Systems
@article{INSYTE,
author = {Porter, Zoe and Calinescu, Radu and Lim, Ernest and Hodge, Victoria and Ryan, Philippa and Burton, Simon and Habli, Ibrahim and Lawton, Tom and McDermid, John and Molloy, John and Monkhouse, Helen and Morgan, Phillip and Noordhof, Paul and Paterson, Colin and Standen, Isobel and Zou, Jie},
title = {INSYTE: A Classification Framework for Traditional to Agentic AI Systems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3760424},
doi = {10.1145/3760424},
abstract = {Existing classification frameworks for artificial intelligence (AI) and autonomous systems are being outpaced by recent advancements in AI technologies. This limits their applicability to modern intelligent systems, particularly agentic AI systems (autonomous systems that leverage foundation models to achieve wide-ranging, multi-layered goals). To address this deficiency, we introduce INSYTE, a multi-faceted framework that supports the classification of AI systems ranging from traditional rule-based systems to cutting-edge embodied AI and agentic systems. To that end, INSYTE considers the essential characteristics of an AI system across eight key dimensions grouped into four categories: system design (underspecification and adaptiveness); functionality (breadth and depth); operating environment (diversity and dynamism); and independence from human operational control (intervention and oversight). Different AI systems (or versions of systems) yield different “patterns” on an eight-axis radar chart that INSYTE uses to provide an immediate visual summary of an AI system's overall capability, and a detailed representation of its individual characteristics. The INSYTE framework aligns with OECD's definition of deployed AI systems, which is becoming the standard definition used by legislators and developers worldwide.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
keywords = {AI system, AI-enabled system, autonomous system, artificial intelligence, agentic AI, taxonomy, classification framework},
annote={This source, released recently on September 13th, 2025, brings an updated, multi-faceted framework to support the continued classification of AI systems, calling it INSYTE. This is to properly define and help differ classic rule-based systems from newer agentic systems, and other cutting-edge technology regarding the use of AI. The INSYTE framework involves eight essential characteristics, also entailing four major groups: system design over under-specification and adaptiveness, system functionality that involves the breadth and depth, operating environment considering diversity and dynamism, and independence from human operational control regarding intervention and oversight. INSYTE is a framework to help discern a visual representation of the overall capabilities of the AI systems, or various versions of systems, through an eight-axis radar chart, using the terms described above, aligned with the Organisation for Economic Co-operation and Development (OECD's) definition of deployed AI systems. This is due to the OECD's definition becoming the standard used by legislators and developers worldwide. To begin, the traditional framing for highest level AI systems is their 'Levels of Autonomy' (LoA), which has been used to help diverse stakeholder compare systems and create development roadmaps since the 1950's. Now within the modern day, INSYTE aims to move beyond LoA due to their rigidity, insufficient detail, inadequacy for classifying agentic AI, an insufficient ability for precise language definitions, a proneness for misuse, and an inconsistency between sectors (e.g. automotive sector Level 3 AI meaning entirely different capabilities compared to the maritime sector's Level 3 AI, allowing no basis of taxonomy). This paper, from 2019-2024, has been gathering data from health, social care, automotive, maritime, manufacturing, mining, space, agriculture, aviation and quarrying AI and AI-enabled autonomous systems that were prototyped and/or studied by other teams and researchers. They also tested and revised their preliminary framework in two rounds, academic and non-academic stakeholders with varying experience levels garnered from vast arrays of domains and disciplines. The article introduces an online INSYTE tool to the reader eventually, after divulging their new level ratings (0-5) for each dimension of the eight dimensions mentioned prior. This holds relevance to the topic due to being the new framework that will be taken from, at least partially.}
}

% Source 6 - An Agentic Framework for Compliant, Ethical and Trustworthy GenAI Applications in Healthcare
@inproceedings{CAM,
author = {Menezes, Veena Priscilla and Chowdhury, Mohammad Jabed Morshed and Mahmood, Abdun},
title = {An Agentic Framework for Compliant, Ethical and Trustworthy GenAI Applications in Healthcare},
year = {2025},
isbn = {9798400715075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727166.3727191},
doi = {10.1145/3727166.3727191},
abstract = {Recent progress in generative artificial intelligence (GenAI) has yielded significant advancements in healthcare, affecting radiology, medical imaging, drug development, patient diagnostics, and supply chain optimisation. These innovations promise more improved diagnoses and time-saving cost-effectiveness. However, GenAI's rapid implementation poses significant challenges for meeting regulatory, ethical, and trustworthiness standards. These challenges include data privacy issues, reproducibility concerns, algorithmic bias in training data causing disparities in outcomes, and a lack of transparency and explainability. Unresolved, these issues could negatively affect the public's confidence in and perception of GenAI systems. Addressing these challenges, international AI governance frameworks, including the EU AI Act and WHO guidelines, prioritize regulatory adherence, trustworthiness, and the explainability of healthcare AI systems. While such frameworks have expanded, a deficiency remains in translating policy into effective compliance mechanisms. We propose a Compliance Agentic Model (CAM) framework to help organizations comply with GenAI and machine learning (ML)-based solutions. The CAM framework establishes trustworthiness in GenAI applications used in healthcare, ensuring alignment with organizational values and ethical standards to enhance accountability and regulatory adherence.},
booktitle = {Proceedings of the 2025 Australasian Computer Science Week},
pages = {48-54},
numpages = {7},
keywords = {GenAI, Healthcare, Agentic AI, Compliance},
location = {},
series = {ACSW '25},
annote = {This is a source regarding more on how to properly make a framework for an agentic, or generative AI (GenAI) that does not express a deficiency regarding translating natural language processing (NLP) into their equivalent action processes with Large Language Models (LLMs). This time the scope is more regarding the precise worries prevalent in healthcare technologies, ensuring maximum safety, and hopefully transparency, for the healthcare workers and patients altogether. Deep neural networks (DNNs) regarding current models (e.g. OpenAI's GPT 4 and later, etc.) involve three main challenging factors: compliance of the GenAI, ethical use of the GenAI, and trust between the DNN-based system and end-user. This source provides another framework to inter into the mind for helping ensure, in non-sensical-and-unprofessional-terms, there will be no 'Skynet' issue with what is created. Regulatory frameworks are being developed now and have been emerging on a large scale since around 2022, with “The Ethics of AI” published by UNESCO that year. The paper takes more time to divulge into the trust issues within Healthcare about the use of AI, but this is outside the purview of the topic this source was selected for, its framework for safety, and proposed compliant agentic model (CAM) framework being the main pulls. The article's idea came from a literature review regarding related solutions in GenAI, taking from eight sources to build from, and then comparing, their proposed CAM model with. CAM's selling points are the machine learning algorithms, deep learning techniques, and reinforcement learning with added AI feedback. It relies on an ethically conscious Constitution and regulatory requirements and is designed to function as an automated compliance instrument within the Generative AI Development Lifecycle.}
}

% Source 7 - Creating Characteristically Auditable Agentic AI Systems
@inproceedings{audit,
author = {Phiri, Charles Chimwemwe},
title = {Creating Characteristically Auditable Agentic AI Systems},
year = {2025},
isbn = {9798400715891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759355.3759356},
doi = {10.1145/3759355.3759356},
abstract = {Auditability — the ability to reliably record, verify, and analyze agent behavior — is a key requirement for trustworthy multi-agent AI systems. This paper formalizes auditability with a set of eight axioms (Integrity, Coverage, Temporal Coherence, Verifiability, Accessibility, Resource Proportionality, Privacy Compatibility, and Governance Alignment) and extends the theory to incorporate liveness properties (ensuring eventual auditability under fault tolerance and agent failures) and adversarial resilience (game-theoretic resistance to strategic log manipulation). We situate this framework in the context of recent research on accountability, transparency, and traceability in distributed AI, and we identify design patterns that operationalize these auditability axioms in system architecture and agent behavior (including partial observability, asynchronous communication, and incentive constraints). All propositions are stated with formal rigor and proved inline. The resulting formalism provides a foundation for building multi-agent systems that are not only auditable by design but also robust against faults and adversaries, aligning with emerging governance requirements for AI.},
booktitle = {Proceedings of the Intelligent Robotics FAIR 2025},
pages = {1-14},
numpages = {14},
keywords = {Auditability, AI Agent. Multi-agent Systems, Integrity, Coverage, Temporal Coherence, Verifiability, Accessibility, Resource Proportionality, Privacy Compatibility, Governance Alignment, Liveness, Adversarial Resilience, Accountability, Transparency, Traceability, Distributed AI},
location = {},
series = {IntRob '25},
annote = {This is an article to properly define what 'auditability' is for the reasoning of recording, verifying and analyzing agent behaviour. Sources annotated prior have already expressed a positive reaction from end-users regarding the availability of transparent feedback after a prompt has been given to an Agentic AI, but also the diminishing effectiveness of over-explaining. This framework, another addition to help ensure this project takes a safe path. This is meant to be a live-tracking method to allow a foundation for designing multi-agent systems with audibility built-in, alongside robustness against bad actors, and an alignment with ever emerging governance requirements for AI. Normaal methods rely on 'in-the-moment' logging and external control, but this uses eight axioms to expand past this, including: integrity, coverage, temporal coherence, verifiability, accessibility, resource proportionality, privacy compatibility, and governance alignment. These rely working within an ideal scenario, as faults must be considered in real-world distributed systems (e.g. agent crashes, delayed communications, etc.). A theoretical algorithmic approach, called Eventual Coverage (a.k.a a modified version of A2, labelled A2-Live) to counterract this, ensuring through key 'liveness' states (i.e. alive and in a failure-free-state, or dead and in a fault-state) whether to begin its logging process, basically, and what to note. This can be expanded to adversarial and game-theoretic analysis, which is important for managing a multi-agent system, as well as providing some extra security advantages via log suppresion, log manipulation, collusion and evasion. Most of these considerations come ot manage the Agents themselves against each other. Final main points involve the use of earlier eight axioms to create foci for the following of A2's design patterns for auditability: tamper-evident logging infrastructure, redundant obersvers and acknowledgements, time synchronization and logical clocks, on-demand log aggregation and query services, adaptive logging and data management, privacy-preserving audit trails, alignment with governance and compliance. This is primarily for software based AI systems.}
}

% Source 8 - Large Language Model Driven Automated Network Protocol Testing
@inproceedings{network,
author = {Wei, Yunze and Chi, Kaiwen and Du, Shibo and Xie, Xiaohui and Geng, Ziyu and Han, Yuwei and Li, Zhen and Li, Zhanyou and Cui, Yong},
title = {Large Language Model Driven Automated Network Protocol Testing},
year = {2025},
isbn = {9798400720093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744200.3744763},
doi = {10.1145/3744200.3744763},
abstract = {Traditional network protocol testing methods face significant challenges in adapting to rapid protocol evolution. The challenges stem primarily from protocol specification analysis and customized code development for testing. To address this, we propose NeTestLLM, a Large Language Model (LLM)-powered framework that automates protocol testing through two key components: (1) a hybrid test case generator that extracts protocol specifications and produces high-coverage test cases, and (2) a retrieval-feedback-enhanced engine that translates natural language descriptions into executable code. Preliminary evaluations demonstrate that NeTestLLM achieves 94.1\% coverage on protocol specification understanding. A case study with commercial network equipment validates the practical effectiveness of our approach. Our work presents the first LLM-powered framework for automated network protocol testing to keep pace with the rapid evolution of network protocols and standards.},
booktitle = {Proceedings of the 2025 Applied Networking Research Workshop},
pages = {32-38},
numpages = {7},
location = {Madrid, Spain},
series = {ANRW '25},
annote = {A source that provides insights into moving beyond traditional network protocol testing methods, this source provides a Large Language Model (LLM)-powered framework to automate protocol testing called \textit{NeTestLLM}. Boasting a 94.1% coverage rate on protocol specification understanding, and successfully conducting a case study through commercial network equipment, the veritability of this framework seems strong. Despite the challenges faced behind making protocol specifications comprehendible by an LLM, test code being highly specialized, and difficulty behind leveraging the accumulated knowledge of experts, a two-pronged approach was developed. The first prong involves a hybrid test case generation module; leveraging the industry examples and national standards to generate test cases that extracted protocol specifications (e.g. finite state machines) from RFC documents and otherwise. This is important for considering the creation process of this agentic Hub, including the direct testing methods here, and framework behind network-protocols for LLM's.}
}